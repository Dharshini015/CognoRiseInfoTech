                             Artificial neural network



Using the MNIST dataset. In this research project we used the MNIST dataset, a popular collections of 28x28 pixel grayscale images of handwritten numbers
with corresponding labels. The data set serves as a benchmark for various
machine learning tasks.

 

workflow diagram reference:
www.researchgate.net/figure/Workflow-diagram-of-the-artificial-neural-network-algorithm-developed-by-Lancashire-et_fig3_323312622

flowchart:





To start our exploration, we imported the necessary libraries such as TensorFlow, Keras,and Matplotlib. Using these tools, we loaded the MNIST dataset and conducted the studyPreliminary examination of its form and content. This passage provided important information about the basic structure of our input data.


Recognizing the importance of preparing the data for training neural networks, we performed normalizing pixel values ​​between 0 and 1. This pre-processing step is crucial for improvement stability and efficiency of the training process and contributes to uniformity
optimization algorithms.


To suit the requirements of certain neural network architectures, particularly those with dense layers, we flattened the images into 1D arrays. This transformation simplified the input structure, optimizing the model's performance.


We experimented with various optimizers and activation functions in the creation of a neural network model using the Keras library. Initially, we tested the model without any hidden layer.

Very Simple neural network with no hidden layers:

pseudo code:

model = keras.Sequential([
    keras.layers.Dense(10, input_shape=(784,), activation='sigmoid')
])
custom_adadelta_optimizer = keras.optimizers.Adadelta(learning_rate=1.0)
model.compile(optimizer=custom_adadelta_optimizer,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(X_train_flattened, y_train, epochs=10)


When training our model, we used the Adadelta optimizer for 10 epochs. The model performed commendably, achieving an accuracy of 92.5% on the training dataset and 93.8% on the testing dataset. To optimize the performance of the model, we chose a sigmoid activation function, which produced excellent results.Specifically, implementing the sigmoid function resulted in an accuracy of 92.6% in training and a slightly better accuracy of 93.68% in testing. On the other hand, when we experimented with ReLU's leaky activation feature, the model's performance decreased significantly, recording an accuracy of 43.3% during training and 45.80% in testing. These results highlight the effectiveness of the sigmoid activation function in improving the overall performance of our model.

with one hidden layer:

In our single hidden layer model architecture, we examined various activation functions to determine their impact on training and testing accuracy. ReLU's implementation of leak activation resulted in a remarkable training accuracy of 97.8%, with a slightly lower but still impressive testing accuracy of 97.5%. ReLU's parametric activation function showed even higher performance, achieving a remarkable training accuracy of 99.33% and an equally impressive test accuracy of 98.08%. Meanwhile, the standard ReLU activation function also showed excellent performance, offering 98.93% training accuracy and 97.80% testing accuracy.
Notably, ReLU's parametric activation function was the most efficient of the three, producing the highest training accuracy of 99.33% and an impressive testing accuracy of 98.08%. This highlights the importance of the choice of activation function for optimizing the overall performance of the model.


with two hidden layer:
pseudo code:


In our neural network model with two hidden layers, we used the parametric activation function ReLU and adadelta optimizer. The training phase resulted in an exceptional accuracy of 99.48%, demonstrating the high learning ability of the model. During testing, the model maintained a high accuracy of 97.94%, which confirmed its generalization ability.The choice of activation function, which can fit and capture complex patterns in the data, contributed significantly to the overall success of the model.

with three hidden layer:
pseudo code:

model = keras.Sequential([
   keras.layers.Dense(100, input_shape=(784,), activation=keras.layers.PReLU(alpha_initializer='zeros')),
    keras.layers.Dense(50,activation=keras.layers.PReLU(alpha_initializer='zeros')),
    keras.layers.Dense(25,activation=keras.layers.PReLU(alpha_initializer='zeros')),
    keras.layers.Dense(10, activation='sigmoid')
])

When building our neural network model, we increased its complexity by including three hidden layers using the parametric activation function ReLU and adadelta optimizer. The training phase delivered an impressive 99.47% accuracy, reflecting the model's ability to capture complex patterns in the training data set. During testing, the model demonstrated a high accuracy of 97.98%, highlighting its strong ability to generalize beyond training data.Consistent use of the ReLU parametric activation function across multiple hidden layers improved the model's performance in terms of learning and accurately predicting from unseen data.

with 4 hidden layer:
pseudo code:




In the design of our neural network model, we augmented its complexity by introducing four hidden layers, accompanied by the Adadelta optimizer and the parametric ReLU activation function. Throughout the training process, the model exhibited exceptional learning capabilities, achieving a remarkable training accuracy of 99.45%. This high level of accuracy underscores the model's adeptness at capturing intricate patterns within the training dataset. Subsequently, during the testing phase, the
model demonstrated strong generalization, maintaining a commendable accuracy of 97.84%. The combination of the Adadelta optimizer and the ReLU parametric activation function on multiple hidden layers synergistically contributed to the overall performance of the model and highlighted its suitability for both training and accurately predicting results from unpublished data.



As we got highest accuracy in e hidden layer:
 
To optimize a neural network model with three hidden layers, we conducted experiments with multiple optimizers. Using the Adadelta optimizer in combination with ReLU's parametric activation function produced impressive results with a training accuracy of 99.47% and a corresponding testing accuracy of 97.98%. Switching to the RMSprop optimizer maintained excellent performance and achieved a training accuracy of 97.15% and a test accuracy of 97.4%, showing the model's ability to generalize well. Similarly, the SGD optimizer combined with the parametric activation function ReLU produced satisfactory results with a training accuracy of 97.10% and a testing accuracy of 97.35%.
Research on the
Nadam optimizer further improved the model, achieving a training accuracy of 99.08% and a testing accuracy of 97.83%, demonstrating its effectiveness in improving the learning capabilities of the model. While the Adagrad optimizer produced competitive results, it demonstrated a training accuracy of 97.70% and a testing accuracy of 97.15%. These results highlight the varying impact of the choice of optimizer on model performance and highlight the importance of careful tuning of hyperparameters to achieve optimal results.


we got highest accuracy while using adadelta optimizer, parametric relu activation function
 
so we tried differnt learning rate:

In our attempt to refine a neural network model that has maximum accuracy using the Adadelta optimizer and the parametric activation function ReLU, we conducted experiments with different learning rates. With a learning rate of 0.01, the model achieved a training accuracy of 90.08%, with a corresponding testing accuracy of 91.38%.Increasing the learning rate to 0.1 then led to a significant improvement in performance, showing a training accuracy of 96.47% and a testing accuracy of 96.6%. Other experiments with learning rate 1.0 achieved remarkable results, training accuracy increased to 99.48%, and testing accuracy reached an impressive 98.07%. These results highlight the sensitivity of model performance to the choice of learning rate, with higher rates indicating the ability for accelerated learning and higher accuracy on both training and test datasets.







































































